{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "from utils import read_line_number, extract_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reader.BaseReader import worker_init_func\n",
    "from task.TopK import init_ranking_report, calculate_ranking_metric\n",
    "\n",
    "def get_userwise_group(model, group_feature, fairness_control):\n",
    "    eval_data = model.reader.get_eval_dataset()\n",
    "    eval_loader = DataLoader(eval_data, worker_init_fn = worker_init_func,\n",
    "                             batch_size = 1, shuffle = False, pin_memory = False, \n",
    "                             num_workers = eval_data.n_worker)\n",
    "    user_groups = {}\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(eval_loader):\n",
    "            # sample user with record in eval data\n",
    "            if \"no_item\" not in batch_data:\n",
    "                # predict\n",
    "                feed_dict = model.wrap_batch(batch_data)\n",
    "                uid = feed_dict[\"user_UserID\"].reshape(-1).detach().cpu().numpy()[0]\n",
    "                user_groups[uid] = fairness_control.group_dict[uid]\n",
    "    return user_groups\n",
    "\n",
    "def get_userwise_performance(model, at_k_list, phase = 'test'):\n",
    "    model.reader.set_phase(phase)\n",
    "    eval_data = model.reader.get_eval_dataset()\n",
    "    eval_loader = DataLoader(eval_data, worker_init_fn = worker_init_func,\n",
    "                             batch_size = 1, shuffle = False, pin_memory = False, \n",
    "                             num_workers = eval_data.n_worker)\n",
    "    user_results = {}\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(eval_loader):\n",
    "            # sample user with record in eval data\n",
    "            if \"no_item\" not in batch_data:\n",
    "                # predict\n",
    "                feed_dict = model.wrap_batch(batch_data)\n",
    "                out_dict = model.forward(feed_dict, return_prob = True)\n",
    "                pos_preds, neg_preds = out_dict[\"probs\"], out_dict[\"neg_probs\"]\n",
    "                if pos_preds.is_cuda:\n",
    "                    pos_preds = pos_preds.detach().cpu()\n",
    "                    neg_preds = neg_preds.detach().cpu()\n",
    "                # metrics\n",
    "                report = init_ranking_report(at_k_list)\n",
    "                calculate_ranking_metric(pos_preds.view(-1), neg_preds.view(-1), at_k_list, report)\n",
    "                uid = feed_dict[\"user_UserID\"].reshape(-1).detach().cpu().numpy()[0]\n",
    "                user_results[uid] = report\n",
    "    return user_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reader.RecDataReader import RecDataReader\n",
    "from model.fair_rec.FairUserGroupPerformance import FairUserGroupPerformance\n",
    "from model.baselines import *\n",
    "from model.fed_rec import *\n",
    "import os\n",
    "import torch\n",
    "\n",
    "model_name_list = {'MF': 'MF', 'FedMF': 'FedMF', 'FairMF': 'MF', 'F2MF': 'FedMF'}\n",
    "device = -1\n",
    "if device >= 0 and torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device)\n",
    "    torch.cuda.set_device(device)\n",
    "    device = \"cuda:\" + str(device)\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "# params = {'at_k_list': [10,50], 'eval_sample_p': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from reader.RecDataReader import sample_negative\n",
    "class EvalDataReader(RecDataReader):\n",
    "    def get_user_feed_dict(self, uid, phase, n_neg = -1):\n",
    "        if len(self.user_hist[uid]) == 0:\n",
    "            return {\"no_item\": True}\n",
    "        items, responses, times = zip(*self.user_hist[uid])\n",
    "        start, end = self.pos_range[phase][uid]\n",
    "        neg_items = sample_negative([self.get_item_feature(iid, \"ItemID\") for iid in items], self.n_items, n_neg = n_neg)\n",
    "        items = items[start:end]\n",
    "        user_data = {\"resp\": np.array(responses[start:end])}\n",
    "        for k,v in self.get_user_meta(uid).items():\n",
    "            user_data[\"user_\" + k] = np.array(v)\n",
    "        if len(items) > 0:\n",
    "            for k,v in self.get_item_list_meta(items).items():\n",
    "                user_data[\"item_\" + k] = np.array(v)\n",
    "            for k,v in self.get_item_list_meta(neg_items, from_idx = True).items():\n",
    "                user_data[\"negi_\" + k] = np.array(v)\n",
    "        else:\n",
    "            user_data[\"no_item\"] = True\n",
    "        return user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Fairness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_feature = 'activity'\n",
    "phase = 'test'\n",
    "modelName = 'FairMF'\n",
    "\n",
    "data_key = 'ml-1m'\n",
    "# data_key = 'amz_Movies_and_TV'\n",
    "# data_key = 'amz_Books'\n",
    "\n",
    "from data.preprocess import ROOT\n",
    "best_setting = {\n",
    "    'ml-1m': {\n",
    "        'MF': [\n",
    "#             '/logs/f2rec_train_and_eval_MF_lr0.0001_reg0.1_losspairwisebpr.log'\n",
    "        ], \n",
    "        'FedMF': [\n",
    "#             '/logs/f2rec_train_and_eval_FedMF_lr0.003_reg0.1_losspairwisebpr_local1_fedavg.log'\n",
    "        ], \n",
    "        'FairMF': [\n",
    "            f'/logs/f2rec_train_and_eval_FairMF_lr0.00003_reg0.1_losspairwisebpr_lambda-0.7_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairMF_lr0.00003_reg0.1_losspairwisebpr_lambda-0.5_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairMF_lr0.00003_reg0.1_losspairwisebpr_lambda-0.3_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairMF_lr0.00003_reg0.1_losspairwisebpr_lambda-0.1_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairMF_lr0.00003_reg0.1_losspairwisebpr_lambda0.1_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairMF_lr0.00003_reg0.1_losspairwisebpr_lambda0.3_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairMF_lr0.00003_reg0.1_losspairwisebpr_lambda0.5_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairMF_lr0.00003_reg0.1_losspairwisebpr_lambda0.7_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairMF_lr0.00003_reg0.1_losspairwisebpr_lambda0.9_g{group_feature}.log'\n",
    "        ],\n",
    "        'F2MF': [\n",
    "            f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda-0.7_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda-0.5_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda-0.3_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda-0.1_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda0.1_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda0.3_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda0.5_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda0.7_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda0.9_sigma0_g{group_feature}.log'\n",
    "#             f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda-0.7_sigma0.01_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda-0.5_sigma0.01_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda-0.3_sigma0.01_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda-0.1_sigma0.01_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda0.1_sigma0.01_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda0.3_sigma0.01_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda0.5_sigma0.01_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda0.7_sigma0.01_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg0.1_losspairwisebpr_lambda0.9_sigma0.01_g{group_feature}.log'\n",
    "        ]\n",
    "    },\n",
    "    'amz_Movies_and_TV': {\n",
    "        'MF': [\n",
    "#             '/logs/f2rec_train_and_eval_MF_lr0.00003_reg1.0_losspairwisebpr.log'\n",
    "        ], \n",
    "        'FedMF': [\n",
    "#             '/logs/f2rec_train_and_eval_FedMF_lr0.003_reg1.0_losspairwisebpr_local1_fedavg.log'\n",
    "        ], \n",
    "        'FairMF': [\n",
    "            '/logs/f2rec_train_and_eval_FairMF_lr0.00001_reg1.0_losspairwisebpr_lambda-0.7_gactivity.log'\n",
    "            ,'/logs/f2rec_train_and_eval_FairMF_lr0.00001_reg1.0_losspairwisebpr_lambda-0.5_gactivity.log'\n",
    "            ,'/logs/f2rec_train_and_eval_FairMF_lr0.00001_reg1.0_losspairwisebpr_lambda-0.3_gactivity.log'\n",
    "            ,'/logs/f2rec_train_and_eval_FairMF_lr0.00001_reg1.0_losspairwisebpr_lambda-0.1_gactivity.log'\n",
    "            ,'/logs/f2rec_train_and_eval_FairMF_lr0.00001_reg1.0_losspairwisebpr_lambda0.1_gactivity.log'\n",
    "            ,'/logs/f2rec_train_and_eval_FairMF_lr0.00001_reg1.0_losspairwisebpr_lambda0.3_gactivity.log'\n",
    "            ,'/logs/f2rec_train_and_eval_FairMF_lr0.00001_reg1.0_losspairwisebpr_lambda0.5_gactivity.log'\n",
    "            ,'/logs/f2rec_train_and_eval_FairMF_lr0.00001_reg1.0_losspairwisebpr_lambda0.7_gactivity.log'\n",
    "            ,'/logs/f2rec_train_and_eval_FairMF_lr0.00001_reg1.0_losspairwisebpr_lambda0.9_gactivity.log'\n",
    "        ],\n",
    "        'F2MF': [\n",
    "            f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda-0.7_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda-0.5_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda-0.3_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda-0.1_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda0.1_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda0.3_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda0.5_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda0.7_sigma0_g{group_feature}.log'\n",
    "            ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda0.9_sigma0_g{group_feature}.log'\n",
    "#             f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda-0.7_sigma0.001_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda-0.5_sigma0.001_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda-0.3_sigma0.001_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda-0.1_sigma0.001_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda0.1_sigma0.001_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda0.3_sigma0.001_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda0.5_sigma0.001_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda0.7_sigma0.001_g{group_feature}.log'\n",
    "#             ,f'/logs/f2rec_train_and_eval_FairFedMF_lr0.003_reg1.0_losspairwisebpr_lambda0.9_sigma0.001_g{group_feature}.log'\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "measures = ['HR','P','RECALL','F1','NDCG']\n",
    "k_list = [1,5,10,20,50]\n",
    "metrics = [f'{m}@{k}' for m in measures for k in k_list] + ['AUC']\n",
    "# for data_key in data_key_list:\n",
    "result_file_path = ROOT + data_key + \"/results/fairness_\" + group_feature + \"_\" + modelName + \"_\" + phase + \".csv\"\n",
    "# result_file_path = ROOT + data_key + \"/results/fairness_\" + group_feature + \"_\" + datetime.datetime.now().strftime('%Y%m%d_%H%M%S') + \".csv\"\n",
    "with open(result_file_path, 'w') as fout:\n",
    "    count = 0\n",
    "#     for modelName, modelDef in model_name_list.items():\n",
    "    modelDef = model_name_list[modelName]\n",
    "    log_path_list = best_setting[data_key][modelName]\n",
    "    for log_path in log_path_list:\n",
    "        # args\n",
    "        try:\n",
    "            args = extract_args(ROOT + data_key + log_path)\n",
    "            print(args)\n",
    "        except:\n",
    "            print('skip')\n",
    "            continue\n",
    "        # reader\n",
    "        if count == 0:\n",
    "            reader = EvalDataReader(args)\n",
    "            reader.n_neg = -1\n",
    "            reader.n_neg_val = -1\n",
    "            reader.n_neg_test = -1\n",
    "            # fairness calculator\n",
    "            args.fair_group_feature = group_feature\n",
    "            fairness_controller = FairUserGroupPerformance(args, reader)\n",
    "        # model\n",
    "        modelClass = eval('{0}.{0}'.format(modelDef))\n",
    "        model = modelClass(args, reader, device)\n",
    "        model.load_from_checkpoint(args.model_path, with_optimizer = False)\n",
    "        model = model.to(device)\n",
    "        model.device = device\n",
    "        # header\n",
    "        if count == 0:\n",
    "            uG = get_userwise_group(model, group_feature, fairness_controller)\n",
    "            fout.write('\\t'.join(['model','fair_group','fair_lambda','metric'] + [str(uid) for uid in uG.keys()]) + '\\n')\n",
    "            fout.write('\\t'.join(['all','-','-','group'] + [str(g) for g in uG.values()]) + '\\n')\n",
    "        count += 1\n",
    "        # evaluation\n",
    "        user_results = get_userwise_performance(model, k_list, phase)\n",
    "        for m in metrics:\n",
    "            fout.write('\\t'.join([modelName,args.fair_group_feature,str(args.fair_lambda),m] + \n",
    "                                 [str(user_results[uid][m]) if uid in user_results else '0' for uid in uG]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_path = ROOT + data_key + \"/results/fairness_\" + group_feature + \"_\" + modelName + \"_\" + phase + \".csv\"\n",
    "with open(result_file_path, 'r') as fin:\n",
    "    header = fin.readline().strip().split('\\t')\n",
    "    result_dict = {i: [h] for i,h in enumerate(header)}\n",
    "    for line in fin:\n",
    "        row = line.strip().split('\\t')\n",
    "        for i,v in enumerate(row):\n",
    "            result_dict[i].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(result_dict,orient='index')\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "groups = np.unique(df[1].values[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "group_metrics = {G: {} for G in groups}\n",
    "for G in tqdm(groups):\n",
    "    subset = df.loc[df[1] == G]\n",
    "    \n",
    "    for col in range(2,len(df.columns)):\n",
    "        label = '-'.join([df[col].iloc[i] for i in range(4)])\n",
    "        group_metrics[G][label] = np.mean([float(v) for v in subset[col].values])\n",
    "print(group_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_diff(groupwise_performance, rho = 1):\n",
    "    S = []\n",
    "    for i,v0 in enumerate(groupwise_performance):\n",
    "        for v1 in groupwise_performance[i+1:]:\n",
    "            S.append(abs(v0-v1) ** rho)\n",
    "    return np.mean(S)\n",
    "\n",
    "for label in group_metrics[groups[0]]:\n",
    "    groupwise_performance = [group_metrics[G][label] for G in groups]\n",
    "    print(f\"{label}\\t: {get_diff(groupwise_performance)}\\t {groupwise_performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Metrics over Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "groups = np.unique(df[1].values[4:])\n",
    "sorted_lambda = sorted(list(set([float(v) for v in df.iloc[2][2:]])))\n",
    "print(f\"lambda:{sorted_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_row = df.iloc[2]\n",
    "metric_row = df.iloc[3]\n",
    "# {metric: {group_feature: [value]}}\n",
    "metric_results = {m: {G: np.zeros(len(sorted_lambda)) for G in groups} for m in metrics}\n",
    "group_size = {}\n",
    "for G in groups:\n",
    "    subset = df.loc[df[1] == G]\n",
    "    group_size[G] = len(subset)\n",
    "    for m, group_results in metric_results.items():\n",
    "        for i,v in enumerate(metric_row):\n",
    "            if v == m:\n",
    "                lbd = float(lambda_row[i])\n",
    "                group_results[G][sorted_lambda.index(lbd)] = np.mean([float(v) for v in subset[i]])\n",
    "for m, group_results in metric_results.items():\n",
    "    group_results['All'] = np.zeros(len(sorted_lambda))\n",
    "    for G,C in group_size.items():\n",
    "        group_results['All'] += group_results[G] * C\n",
    "    group_results['All'] /= len(df)\n",
    "metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_multiple_line(stats, features, x_ticks = [], ncol = 2, row_height = 4, no_title = False, no_xticks = False,\n",
    "                       ylabel = 'y', xlabel = 'x', legend_title = '', legend_appear_at = 0):\n",
    "    '''\n",
    "    @input:\n",
    "    - stats: {field_name: {key: [values]}}\n",
    "    - features: [field_name]\n",
    "    - ncol: number of subplots in each row\n",
    "    '''\n",
    "    assert ncol > 0\n",
    "    N = len(features)\n",
    "    fig_height = 12 // ncol if len(features) == 1 else row_height*((N-1)//ncol+1)\n",
    "    plt.figure(figsize = (16, fig_height))\n",
    "    for i,field in enumerate(features):\n",
    "        plt.subplot((N-1)//ncol+1,ncol,i+1)\n",
    "        minY,maxY = float('inf'),float('-inf')\n",
    "        for key, value_list in stats[field].items():\n",
    "#             print(key, value_list)\n",
    "            X = np.arange(1,len(value_list)+1) if len(x_ticks) == 0 else x_ticks\n",
    "            minY,maxY = min(minY,min(value_list)),max(maxY,max(value_list))\n",
    "            if i == legend_appear_at:\n",
    "                plt.plot(X,value_list,label = key)\n",
    "            else:\n",
    "                plt.plot(X,value_list)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel(xlabel)\n",
    "        if not no_title:\n",
    "            plt.title(field)\n",
    "        if no_xticks:\n",
    "            plt.xticks([])\n",
    "        scale = 1e-4 + maxY - minY\n",
    "        plt.ylim(minY - scale * 0.05, maxY + scale * 0.05)\n",
    "        if i == legend_appear_at:\n",
    "            plt.legend(title = legend_title, loc = 'center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 18\n",
    "# selected_metrics = list(metric_results.keys())\n",
    "# selected_metrics = ['P@10','RECALL@10', 'AUC']\n",
    "# selected_metrics = [f'RECALL@{k}' for k in [1,10,50]]\n",
    "# selected_metrics = [f'F1@{k}' for k in [1,10,50]]\n",
    "# selected_metrics = ['F1@50']\n",
    "for m in [f'{m_name}@{k}' for m_name in ['F1','RECALL'] for k in [1,10,50]]:\n",
    "    selected_metrics = [m]\n",
    "    plot_multiple_line(metric_results, selected_metrics, x_ticks = sorted_lambda, \n",
    "                       no_title = True, no_xticks = True,\n",
    "                       ncol = 3, ylabel = '', xlabel = '', \n",
    "                       legend_title = 'activity', legend_appear_at = -1)\n",
    "#     plot_multiple_line(metric_results, selected_metrics, x_ticks = sorted_lambda, \n",
    "#                        ncol = 3, ylabel = '', xlabel = '', \n",
    "#                        legend_title = 'activity', legend_appear_at = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Performance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "pick_column = {'model_name': }\n",
    "label = '-'.join([df[pick_column].iloc[i] for i in range(4)])\n",
    "group_metrics = {G: {} for G in groups}\n",
    "for G in tqdm(groups):\n",
    "    subset = df.loc[df[1] == G]\n",
    "    group_metrics[G][label] = [float(v) for v in subset[pick_column].values if float(v) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for G in groups:\n",
    "    ax = sns.distplot(group_metrics[G][label], rug=True, hist=False, label = G)\n",
    "#     sns.histplot(data=group_metrics[G][label], x = G, kde=True)\n",
    "#     plt.hist(group_metrics[G][label], 50, density = True, label = G)\n",
    "plt.title(label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BMRL",
   "language": "python",
   "name": "bmrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
