{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Steps\n",
    "\n",
    "* Build vocab files for user and item, textline format: field  value  idx\n",
    "* Filter multicore rating data\n",
    "* Holdout train-val-test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "DATA_ROOT = \"/home/sl1471/public/\"\n",
    "PROCESSED_DATA_ROOT = \"/home/sl1471/workspace/experiments/\"\n",
    "data_path = DATA_ROOT + \"amazon_rating_only/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data file name, main_cat field in meta\n",
    "# domain = [\"Books\", \"Books\", 80, 30]\n",
    "# domain = [\"Clothing_Shoes_and_Jewelry\", \"Clothing_Shoes_Jewelry\", 25, 5]\n",
    "# domain = [\"Home_and_Kitchen\", \"Home_and_Kitchen\", 25, 5]\n",
    "# domain = [\"Electronics\", \"Electronics\", 30, 5]\n",
    "# domain = [\"Sports_and_Outdoors\", \"Sports\", 10, 5]\n",
    "domain = [\"Movies_and_TV\", \"Movies_and_TV\", 40, 10]\n",
    "# domain = [\"Video_Games\", \"Video_Games\", 15,  5]\n",
    "\n",
    "\n",
    "target_path = PROCESSED_DATA_ROOT + \"amz_\" + domain[0] + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Response</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001527665</td>\n",
       "      <td>A3478QRKQDOPQ2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1362960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001527665</td>\n",
       "      <td>A2VHSG6TZHU1OB</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1361145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001527665</td>\n",
       "      <td>A23EJWOW1TLENE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1358380800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID          UserID  Response   Timestamp\n",
       "0  0001527665  A3478QRKQDOPQ2       5.0  1362960000\n",
       "1  0001527665  A2VHSG6TZHU1OB       5.0  1361145600\n",
       "2  0001527665  A23EJWOW1TLENE       5.0  1358380800"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_table(data_path + domain[0] + \".csv\", sep=\",\", names = [\"ItemID\", \"UserID\", \"Response\", \"Timestamp\"])\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#user: 3826085\n",
      "#item: 182032\n",
      "sparsity: 0.9999874142903511\n"
     ]
    }
   ],
   "source": [
    "n_user, n_item = len(df.UserID.unique()), len(df.ItemID.unique())\n",
    "print(f\"#user: {n_user}\")\n",
    "print(f\"#item: {n_item}\")\n",
    "print(f\"sparsity: {1.0 - len(df) / (n_user * n_item)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-core is set to [5,100]\n",
      "Filtering (40,10)-core data\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8765568/8765568 [00:15<00:00, 573537.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 8031261\n",
      "Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 734307/734307 [00:00<00:00, 802205.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 167037\n",
      "Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 567270/567270 [00:01<00:00, 552201.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 56148\n",
      "Iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 511122/511122 [00:00<00:00, 738266.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 16655\n",
      "Iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 494467/494467 [00:00<00:00, 503451.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 5829\n",
      "Iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488638/488638 [00:00<00:00, 540113.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 2257\n",
      "Iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 486381/486381 [00:00<00:00, 809299.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 1041\n",
      "Iteration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 485340/485340 [00:00<00:00, 822322.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 560\n",
      "Iteration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 484780/484780 [00:00<00:00, 913801.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 332\n",
      "Iteration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 484448/484448 [00:00<00:00, 773351.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 216\n",
      "Iteration 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 484232/484232 [00:00<00:00, 915767.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 88\n",
      "Iteration 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 484144/484144 [00:00<00:00, 892795.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 3\n",
      "Iteration 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 484141/484141 [00:00<00:00, 712919.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 0\n",
      "Size change: 8765568 --> 484141\n"
     ]
    }
   ],
   "source": [
    "from data.preprocess import run_multicore, run_multicore_asymetric\n",
    "# multicore_data = run_multicore(df[[\"UserID\", \"ItemID\", \"Response\", \"Timestamp\"]], n_core = domain[2])\n",
    "multicore_data = run_multicore_asymetric(df[[\"UserID\", \"ItemID\", \"Response\", \"Timestamp\"]], n_core_user = domain[2], n_core_item = domain[3])\n",
    "# multicore_data = run_multicore_asymetric(multicore_data, n_core_user = 40, n_core_item = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#user: 5515\n",
      "#item: 13509\n",
      "#record: 484141\n",
      "sparsity: 0.9935016493151505\n"
     ]
    }
   ],
   "source": [
    "n_user, n_item = len(multicore_data.UserID.unique()), len(multicore_data.ItemID.unique())\n",
    "print(f\"#user: {n_user}\")\n",
    "print(f\"#item: {n_item}\")\n",
    "print(f\"#record: {len(multicore_data)}\")\n",
    "print(f\"sparsity: {1.0 - len(multicore_data) / (n_user * n_item)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13509\n"
     ]
    }
   ],
   "source": [
    "items = {iid: False for iid in multicore_data['ItemID'].unique()}\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15138777it [00:30, 498931.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item meta info of items in data set:\n",
      "Found: 13493\n",
      "Missing: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# pd.read_csv(data_path + \"meta/filtered_meta.csv\", sep = '\\t', nrows = 3)\n",
    "item_meta = {}\n",
    "with open(data_path + \"meta/filtered_meta.csv\", 'r') as fin:\n",
    "    fin.readline()\n",
    "    for i,line in tqdm(enumerate(fin)):\n",
    "        meta_info = line.strip().split(\"\\t\")\n",
    "        item_id = meta_info[0]\n",
    "        if item_id in items:\n",
    "            item_meta[item_id] = meta_info\n",
    "            del items[item_id]\n",
    "            if len(items) == 0:\n",
    "                break\n",
    "print(\"Item meta info of items in data set:\")\n",
    "print(f\"Found: {len(item_meta)}\")\n",
    "print(f\"Missing: {len(items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-core is set to [5,100]\n",
      "Filtering (40,10)-core data\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483529/483529 [00:00<00:00, 724512.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 352\n",
      "Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483177/483177 [00:00<00:00, 628045.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 58\n",
      "Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483119/483119 [00:00<00:00, 1093326.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 89\n",
      "Iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483030/483030 [00:00<00:00, 922976.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 259\n",
      "Iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482771/482771 [00:00<00:00, 875666.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 77\n",
      "Iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482694/482694 [00:00<00:00, 904587.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 56\n",
      "Iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482638/482638 [00:00<00:00, 782859.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 14\n",
      "Iteration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482624/482624 [00:00<00:00, 641878.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 47\n",
      "Iteration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482577/482577 [00:00<00:00, 809491.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 58\n",
      "Iteration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482519/482519 [00:00<00:00, 640432.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 8\n",
      "Iteration 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482511/482511 [00:00<00:00, 803271.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed record: 0\n",
      "Size change: 483529 --> 482511\n"
     ]
    }
   ],
   "source": [
    "selected_rows = [True] * len(multicore_data)\n",
    "for i,iid in enumerate(multicore_data[\"ItemID\"]):\n",
    "    if iid in items:\n",
    "        selected_rows[i] = False\n",
    "multicore_data = multicore_data[selected_rows]\n",
    "multicore_data = run_multicore_asymetric(multicore_data, domain[2], domain[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#user: 5497\n",
      "#item: 13457\n",
      "#record: 482511\n",
      "sparsity: 0.9934772125159124\n"
     ]
    }
   ],
   "source": [
    "n_user, n_item = len(multicore_data.UserID.unique()), len(multicore_data.ItemID.unique())\n",
    "print(f\"#user: {n_user}\")\n",
    "print(f\"#item: {n_item}\")\n",
    "print(f\"#record: {len(multicore_data)}\")\n",
    "print(f\"sparsity: {1.0 - len(multicore_data) / (n_user * n_item)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "multicore_data = multicore_data.sort_values(by=['UserID','Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build user history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "482511it [00:01, 381098.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout user histories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5497/5497 [00:00<00:00, 8266.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move unseen ItemID from val to train\n",
      "0/46010, finish in 1230793476.1s.   \r",
      "Moving user data\n",
      "Before moving: Target DataFrame: 390491, Source Data Frame: 46010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 2944.20it/s]\n",
      "100%|██████████| 5492/5492 [00:01<00:00, 5459.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#user moved: 5\n",
      "After moving: Target DataFrame: 390599, Source Data Frame: 45902\n",
      "Move unseen ItemID from test to train, this may also move users in val to train\n",
      "0/46010, finish in 1468834638.6s.   \r",
      "Val --> Train\n",
      "Moving user data\n",
      "Before moving: Target DataFrame: 390599, Source Data Frame: 45902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 2815.91it/s]\n",
      "100%|██████████| 5480/5480 [00:00<00:00, 6634.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#user moved: 12\n",
      "After moving: Target DataFrame: 390698, Source Data Frame: 45803\n",
      "Test --> Train\n",
      "Moving user data\n",
      "Before moving: Target DataFrame: 390698, Source Data Frame: 46010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 3999.97it/s]\n",
      "100%|██████████| 5485/5485 [00:01<00:00, 5172.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#user moved: 12\n",
      "After moving: Target DataFrame: 390797, Source Data Frame: 45911\n"
     ]
    }
   ],
   "source": [
    "from utils import set_random_seed\n",
    "set_random_seed(9)\n",
    "from data.preprocess import holdout_data_sequential, recheck_exist\n",
    "trainset, valset, testset = holdout_data_sequential(multicore_data, holdout_type = \"warm\", ratio = [0.8,0.1,0.1])\n",
    "trainset = trainset.reset_index(drop = True)\n",
    "valset = valset.reset_index(drop = True)\n",
    "testset = testset.reset_index(drop = True)\n",
    "# recheck if there is any unseen item in val or test, if there is move corresponding user history into train\n",
    "trainset, valset, testset = recheck_exist(trainset, valset, testset, field_name = \"ItemID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error when creating \"\"\n",
      "dir \"/home\" existed\n",
      "dir \"/home/sl1471\" existed\n",
      "dir \"/home/sl1471/workspace\" existed\n",
      "dir \"/home/sl1471/workspace/experiments\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV/tsv_data\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV/tsv_data/\" existed\n"
     ]
    }
   ],
   "source": [
    "from utils import setup_path\n",
    "save_path = target_path + \"tsv_data/\"\n",
    "setup_path(save_path, is_dir = True)\n",
    "trainset.to_csv(save_path + \"train.tsv\", sep = '\\t', index = False)\n",
    "valset.to_csv(save_path + \"val.tsv\", sep = '\\t', index = False)\n",
    "testset.to_csv(save_path + \"test.tsv\", sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5497, 5480, 5485)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset[\"UserID\"].unique()), len(valset[\"UserID\"].unique()), len(testset[\"UserID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13457, 11112, 10994)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset[\"ItemID\"].unique()), len(valset[\"ItemID\"].unique()), len(testset[\"ItemID\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Data\n",
    "\n",
    "See fields description [here](https://nijianmo.github.io/amazon/index.html)\n",
    "\n",
    "15023058 lines\n",
    "\n",
    "Fields: 'category', 'tech1', 'description', 'fit', 'title', 'also_buy', 'image', 'tech2', 'brand', 'feature', 'rank', 'also_view', 'details', 'main_cat', 'similar_item', 'date', 'price', 'asin'\n",
    "\n",
    "Selected fields: \"asin\", \"category\", \"price\", \"brand\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_fields = [\"asin\", \"category\", \"price\", \"also_buy\", \"also_view\", \"brand\"]\n",
    "selected_fields = [\"asin\", \"category\", \"price\", \"brand\"]\n",
    "# import numpy as np\n",
    "\n",
    "# def price_to_id(price):\n",
    "#     if price == '':\n",
    "#         return 0\n",
    "#     p = price.strip().replace('$','').split('.')\n",
    "#     l,v = len(p[0]), int(p[0][0])\n",
    "# #     return (l-1) * 3 + (0 if v<3 else 1 if v < 6 else 2) + 1\n",
    "#     return (l-1) * 10 + v + 1\n",
    "\n",
    "# def clean(textline):\n",
    "#     return textline.replace(',','_').replace(' ','').replace('&','_').replace('\\t','')\n",
    "\n",
    "# with open(data_path + \"meta/filtered_meta.csv\", 'w') as fout:\n",
    "#     fout.write(\"ItemID\\tCategory\\tMin_Price\\tMax_Price\\tAlso_Buy\\tAlso_View\\tBrand\\n\")\n",
    "#     with open(data_path + \"meta/All_Amazon_Meta.json\", 'r') as fin:\n",
    "#         for i,line in enumerate(fin):\n",
    "#             if i % 100000 == 0:\n",
    "#                 print(f\"#line: {i}\", end = '\\r')\n",
    "#             info = eval(line)\n",
    "#             fout.write(info['asin'] + \"\\t\") # ItemID\n",
    "#             fout.write(\",\".join([clean(c) for c in info['category'] \\\n",
    "#                                  if len(c) < 30]) + \"\\t\") # Category\n",
    "#             try:\n",
    "#                 if len(info['price']) > 20:\n",
    "#                     fout.write(\"0\\t0\\t\")\n",
    "#                 else:\n",
    "#                     price = info['price'].split(\"-\") # Min_Price and Max_Price\n",
    "#                     if len(price) == 2:\n",
    "#                         fout.write(f\"{price_to_id(price[0])}\\t{price_to_id(price[1])}\\t\")\n",
    "#                     else:\n",
    "#                         pidx = price_to_id(price[0])\n",
    "#                         fout.write(f\"{pidx}\\t{pidx}\\t\")\n",
    "#             except:\n",
    "#                 fout.write(\"0\\t0\\t\")\n",
    "#             fout.write(\",\".join(info['also_buy']) + \"\\t\")\n",
    "#             fout.write(\",\".join(info['also_view']) + \"\\t\")\n",
    "#             fout.write(clean(info['brand'][:30]) + \"\\n\")\n",
    "#     print(f\"#line: {i}\", end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Min_Price</th>\n",
       "      <th>Max_Price</th>\n",
       "      <th>Also_Buy</th>\n",
       "      <th>Also_View</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6305121869</td>\n",
       "      <td>Clothing_Shoes_Jewelry,Women,Clothing,Tops_Tee...</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ninasill_Blouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6318708057</td>\n",
       "      <td>Clothing_Shoes_Jewelry,Traditional_CulturalWea...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coolred-Women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6342506256</td>\n",
       "      <td>Clothing_Shoes_Jewelry,Men,Clothing,Shorts,Car...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B07CRJ95M7,B008AHISU4,B07B8F98W2,B07DD98Q7R,B0...</td>\n",
       "      <td>Gaok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID                                           Category  Min_Price  \\\n",
       "0  6305121869  Clothing_Shoes_Jewelry,Women,Clothing,Tops_Tee...         10   \n",
       "1  6318708057  Clothing_Shoes_Jewelry,Traditional_CulturalWea...         12   \n",
       "2  6342506256  Clothing_Shoes_Jewelry,Men,Clothing,Shorts,Car...         13   \n",
       "\n",
       "   Max_Price  Also_Buy                                          Also_View  \\\n",
       "0         12       NaN                                                NaN   \n",
       "1         12       NaN                                                NaN   \n",
       "2         13       NaN  B07CRJ95M7,B008AHISU4,B07B8F98W2,B07DD98Q7R,B0...   \n",
       "\n",
       "             Brand  \n",
       "0  Ninasill_Blouse  \n",
       "1    Coolred-Women  \n",
       "2             Gaok  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(data_path + \"meta/filtered_meta.csv\", sep = '\\t', nrows = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13457\n"
     ]
    }
   ],
   "source": [
    "items = {iid: False for iid in multicore_data['ItemID'].unique()}\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12178103it [00:26, 467909.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item meta info of items in data set:\n",
      "Found: 13457\n",
      "Missing: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "item_meta = {}\n",
    "with open(data_path + \"meta/filtered_meta.csv\", 'r') as fin:\n",
    "    fin.readline()\n",
    "    for i,line in tqdm(enumerate(fin)):\n",
    "        meta_info = line.strip().split(\"\\t\")\n",
    "        item_id = meta_info[0]\n",
    "        if item_id in items:\n",
    "            item_meta[item_id] = meta_info\n",
    "            del items[item_id]\n",
    "            if len(items) == 0:\n",
    "                break\n",
    "print(\"Item meta info of items in data set:\")\n",
    "print(f\"Found: {len(item_meta)}\")\n",
    "print(f\"Missing: {len(items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Category</th>\n",
       "      <th>MinPrice</th>\n",
       "      <th>MaxPrice</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005019281</td>\n",
       "      <td>Movies_TV,Movies</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005119367</td>\n",
       "      <td>Movies_TV,StudioSpecials,WarnerHomeVideo,AllTi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BenKingsley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006486576</td>\n",
       "      <td>Movies_TV,ChristianVideo,General</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>BrianDeacon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID                                           Category MinPrice  \\\n",
       "0  0005019281                                   Movies_TV,Movies        0   \n",
       "1  0005119367  Movies_TV,StudioSpecials,WarnerHomeVideo,AllTi...        0   \n",
       "2  0006486576                   Movies_TV,ChristianVideo,General        5   \n",
       "\n",
       "  MaxPrice        Brand  \n",
       "0        0         None  \n",
       "1        0  BenKingsley  \n",
       "2        5  BrianDeacon  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "item_meta_df = pd.DataFrame.from_dict(item_meta, orient = \"index\", columns = [\"ItemID\", \"Category\", \"MinPrice\", \"MaxPrice\", \"AlsoBuy\", \"AlsoView\", \"Brand\"])\n",
    "item_meta_df = item_meta_df[[\"ItemID\", \"Category\", \"MinPrice\", \"MaxPrice\", \"Brand\"]]\n",
    "item_meta_df = item_meta_df.reset_index(drop = True)\n",
    "# item_meta_df = pd.DataFrame.from_dict(item_meta, orient = \"index\", columns = [\"ItemID\", \"Category\", \"MinPrice\", \"MaxPrice\", \"Brand\"])\n",
    "# item_meta_df.insert(0, 'ItemID', item_meta_df.index)\n",
    "item_meta_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error when creating \"\"\n",
      "dir \"/home\" existed\n",
      "dir \"/home/sl1471\" existed\n",
      "dir \"/home/sl1471/workspace\" existed\n",
      "dir \"/home/sl1471/workspace/experiments\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV/meta_data\" existed\n"
     ]
    }
   ],
   "source": [
    "from utils import setup_path\n",
    "save_path = target_path + \"meta_data/item.meta\"\n",
    "setup_path(save_path, is_dir = False)\n",
    "item_meta_df.to_csv(save_path, sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error when creating \"\"\n",
      "dir \"/home\" existed\n",
      "dir \"/home/sl1471\" existed\n",
      "dir \"/home/sl1471/workspace\" existed\n",
      "dir \"/home/sl1471/workspace/experiments\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV/meta_data\" existed\n",
      "Vocab file saved to: /home/sl1471/workspace/experiments/amz_Movies_and_TV/meta_data/item_fields.vocab\n"
     ]
    }
   ],
   "source": [
    "from data.preprocess import build_vocab\n",
    "from utils import setup_path\n",
    "save_path = target_path + \"meta_data/item_fields.vocab\"\n",
    "setup_path(save_path, is_dir = False)\n",
    "build_vocab(item_meta_df, save_path, [\"ItemID\", \"Category\", \"MinPrice\", \"MaxPrice\", \"Brand\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# item_fields_meta = pd.DataFrame({\"field_name\": [\"ItemID\", \"Category\", \"MinPrice\", \"MaxPrice\", \"AlsoBuy\", \"AlsoView\", \"Brand\"], \n",
    "#                                  \"field_type\": [\"nominal\", \"nominal\", \"ordinal\", \"ordinal\", \"nominal\", \"nominal\", \"nominal\"], \n",
    "#                                  \"value_type\": [\"int\", \"str\", \"int\", \"int\", \"int\", \"int\", \"str\"], \n",
    "#                                  \"field_enc\": [\"v2id\", \"v2onehot\", \"v2id\", \"v2id\", \"v2multid\", \"v2multid\", \"v2id\"], \n",
    "#                                  \"vocab_key\": [\"ItemID\", \"Category\", \"MinPrice\", \"MaxPrice\", \"ItemID\", \"ItemID\", \"Brand\"]})\n",
    "item_fields_meta = pd.DataFrame({\"field_name\": [\"ItemID\", \"Category\", \"MinPrice\", \"MaxPrice\", \"Brand\"], \n",
    "                                 \"field_type\": [\"nominal\", \"nominal\", \"ordinal\", \"ordinal\", \"nominal\"], \n",
    "                                 \"value_type\": [\"str\", \"str\", \"int\", \"int\", \"str\"], \n",
    "                                 \"field_enc\": [\"v2id\", \"v2onehot\", \"v2id\", \"v2id\", \"v2id\"], \n",
    "                                 \"vocab_key\": [\"ItemID\", \"Category\", \"MinPrice\", \"MaxPrice\", \"Brand\"]})\n",
    "item_fields_meta.to_csv(target_path + \"meta_data/item_fields.meta\", \n",
    "                        sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A100WO06OQR8BQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A10175AMUHOQC4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A101IGU6UDKW3X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           UserID\n",
       "0  A100WO06OQR8BQ\n",
       "1  A10175AMUHOQC4\n",
       "2  A101IGU6UDKW3X"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = {uid: uid for uid in multicore_data['UserID'].unique()}\n",
    "user_meta_df = pd.DataFrame.from_dict(users, orient = \"index\", columns = [\"UserID\"])\n",
    "user_meta_df = user_meta_df.reset_index(drop = True)\n",
    "user_meta_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error when creating \"\"\n",
      "dir \"/home\" existed\n",
      "dir \"/home/sl1471\" existed\n",
      "dir \"/home/sl1471/workspace\" existed\n",
      "dir \"/home/sl1471/workspace/experiments\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV/meta_data\" existed\n"
     ]
    }
   ],
   "source": [
    "from utils import setup_path\n",
    "save_path = target_path + \"meta_data/user.meta\"\n",
    "setup_path(save_path, is_dir = False)\n",
    "user_meta_df.to_csv(save_path, sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error when creating \"\"\n",
      "dir \"/home\" existed\n",
      "dir \"/home/sl1471\" existed\n",
      "dir \"/home/sl1471/workspace\" existed\n",
      "dir \"/home/sl1471/workspace/experiments\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV\" existed\n",
      "dir \"/home/sl1471/workspace/experiments/amz_Movies_and_TV/meta_data\" existed\n",
      "Vocab file saved to: /home/sl1471/workspace/experiments/amz_Movies_and_TV/meta_data/user_fields.vocab\n"
     ]
    }
   ],
   "source": [
    "from data.preprocess import build_vocab\n",
    "from utils import setup_path\n",
    "save_path = target_path + \"meta_data/user_fields.vocab\"\n",
    "setup_path(save_path, is_dir = False)\n",
    "build_vocab(user_meta_df, save_path, [\"UserID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "user_fields_meta = pd.DataFrame({\"field_name\": [\"UserID\"], \n",
    "                          \"field_type\": [\"nominal\"], \n",
    "                          \"value_type\": [\"str\"], \n",
    "                          \"field_enc\": [\"v2id\"], \n",
    "                          \"vocab_key\": [\"UserID\"]})\n",
    "user_fields_meta.to_csv(target_path + \"meta_data/user_fields.meta\", sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BMRL",
   "language": "python",
   "name": "bmrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
